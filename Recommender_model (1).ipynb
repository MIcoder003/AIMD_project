{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vml8HnP-sMZ",
        "outputId": "2bb9d4dd-f88c-494d-b525-2f3b4a923008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting implicit\n",
            "  Using cached implicit-0.7.2.tar.gz (70 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from implicit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.12/dist-packages (from implicit) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from implicit) (3.6.0)\n",
            "Building wheels for collected packages: implicit\n",
            "  Building wheel for implicit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for implicit: filename=implicit-0.7.2-cp312-cp312-linux_x86_64.whl size=10797946 sha256=e5bc1496bf153619df9fa683a262e1a014e317a2cd9731b177bfb67ed9a7ff9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/00/4f/9ff8af07a0a53ac6007ea5d739da19cfe147a2df542b6899f8\n",
            "Successfully built implicit\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "import boto3\n",
        "from sagemaker import get_execution_role"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "fA3hqR89-yYg",
        "outputId": "a1dbb935-9aef-42b0-9d05-ab0ae35b53aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'implicit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-349439939.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlternatingLeastSquares\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'implicit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "dvpJy08cs8cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "\n",
        "conn = boto3.client('s3')\n",
        "bucket_name = 'ecomrecdata'\n",
        "contnet = conn.list_objects(Bucket=bucket_name)\n",
        "\n",
        "category_tree = pd.read_csv(f's3://{bucket_name}/data/category_tree.csv')\n",
        "events = pd.read_csv(f's3://{bucket_name}/data/events.csv')\n",
        "item_props1 = pd.read_csv(f's3://{bucket_name}/data/item_properties_part1.csv')\n",
        "item_props2 = pd.read_csv(f's3://{bucket_name}/data/item_properties_part2.csv')\n",
        "\n",
        "# Sample 10% of events\n",
        "logging.info(f\"Original events shape: {events.shape}\")\n",
        "events = events.sample(frac=0.1, random_state=42)\n",
        "logging.info(f\"Sampled events shape: {events.shape}\")\n"
      ],
      "metadata": {
        "id": "RsnBPN15tU64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZEORlLOst5Q"
      },
      "outputs": [],
      "source": [
        "# 2. Combine & Clean\n",
        "item_properties = pd.concat([item_props1, item_props2], ignore_index=True)\n",
        "events = events.dropna()\n",
        "events['timestamp'] = pd.to_datetime(events['timestamp'], errors='coerce')\n",
        "events = events.dropna(subset=['timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_user_events = 5\n",
        "min_item_events = 5\n",
        "user_counts = events['visitorid'].value_counts()\n",
        "item_counts = events['itemid'].value_counts()\n",
        "events = events[events['visitorid'].isin(user_counts[user_counts >= min_user_events].index)]\n",
        "events = events[events['itemid'].isin(item_counts[item_counts >= min_item_events].index)]\n",
        "logging.info(f\"Filtered events shape: {events.shape}\")"
      ],
      "metadata": {
        "id": "aNBfQsx6s_84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Add weight column based on event type\n",
        "event_weights = {'view': 1, 'addtocart': 3, 'transaction': 5}\n",
        "events['weight'] = events['event'].map(event_weights)"
      ],
      "metadata": {
        "id": "GzzQA8_E_Hea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. User-Item Matrix\n",
        "user_item_matrix = events.pivot_table(\n",
        "    index='visitorid',\n",
        "    columns='itemid',\n",
        "    values='weight',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
        "logging.info(f\"User-item matrix shape: {user_item_matrix.shape}\")"
      ],
      "metadata": {
        "id": "CH5hCtmZ_J5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning model"
      ],
      "metadata": {
        "id": "Y9ZxotiutAe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. ALS Training\n",
        "train_data, test_data = train_test_split(events, test_size=0.2, random_state=42)\n",
        "train_matrix = train_data.pivot_table(index='visitorid', columns='itemid', values='weight', aggfunc='sum', fill_value=0)\n",
        "train_sparse = csr_matrix(train_matrix.values)\n",
        "logging.info(f\"Train matrix shape: {train_matrix.shape}\")\n",
        "\n",
        "# Build user and item maps\n",
        "user_map = dict(enumerate(train_matrix.index))\n",
        "item_map = dict(enumerate(train_matrix.columns))\n",
        "user_inv_map = {v: k for k, v in user_map.items()}\n",
        "item_inv_map = {v: k for k, v in item_map.items()}\n",
        "\n",
        "# Fit ALS\n",
        "model = AlternatingLeastSquares(factors=32, regularization=0.1, iterations=15)\n",
        "model.fit(train_sparse.T)\n",
        "logging.info(\"ALS model trained\")"
      ],
      "metadata": {
        "id": "GYak-YEutCfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Create item_metadata\n",
        "item_properties.sort_values(\"timestamp\", inplace=True)\n",
        "item_properties.drop_duplicates(subset=[\"itemid\", \"property\"], keep=\"last\", inplace=True)\n",
        "item_metadata = item_properties.pivot(index='itemid', columns='property', values='value').reset_index()\n",
        "\n",
        "# Filter item_metadata to items in train_matrix\n",
        "item_metadata = item_metadata[item_metadata['itemid'].isin(train_matrix.columns)]\n",
        "\n",
        "for col in ['brand', 'categoryid', 'color']:\n",
        "    if col not in item_metadata.columns:\n",
        "        item_metadata[col] = ''\n",
        "\n",
        "item_metadata['text'] = item_metadata[['brand', 'categoryid', 'color']].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(item_metadata['text'])\n",
        "item_ids = item_metadata['itemid'].values\n",
        "itemid_to_index = {itemid: idx for idx, itemid in enumerate(item_ids)}"
      ],
      "metadata": {
        "id": "Siq6cgyytCxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving artifacts"
      ],
      "metadata": {
        "id": "M-stgT7gVq1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "import os\n",
        "\n",
        "bucket_name = 'ecomrecdata'\n",
        "s3_prefix = 'data'\n",
        "loca\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(user_inv_map, os.path.join(local_dir, 'user_inv_map.joblib'))\n",
        "joblib.dump(item_map, os.path.join(local_dir, 'item_map.joblib'))\n",
        "joblib.dump(train_sparse.joblib, os.path.join(local_dir, 'train_sparse.joblib'))\n",
        "train_data.to_csv(os.path.join(local_dir, 'train_data.csv'), index=False)\n",
        "joblib.dump(item_id_index, os.path.join(local_dir, 'item_id_index.joblib'))\n",
        "joblib.dump(item_ids, os.path.join(local_dir, 'item_ids.joblib'))\n",
        "joblib.dump(tfidf_matrix, os.path.join(local_dir, 'tfidf_matrix.joblib'))"
      ],
      "metadata": {
        "id": "mo1R_O33VsgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "bucket_name = 'ecomrecdata'\n",
        "s3_prefix = 'data'\n",
        "local_dir = 'temp_model_files'\n",
        "\n",
        "def upload_local_directory_to_s3(local_directory, bucket, s3_prefix):\n",
        "    \"\"\"\n",
        "    Uploads all files in a local directory to a specific prefix in an S3 bucket.\n",
        "    \"\"\"\n",
        "    s3 = boto3.client('s3')\n",
        "\n",
        "    for root, dirs, files in os.walk(local_directory):\n",
        "        for filename in files:\n",
        "            local_path = os.path.join(root, filename)\n",
        "            s3_key = os.path.join(s3_prefix, filename)\n",
        "\n",
        "            print(f\"Uploading {local_path} to s3://{bucket}/{s3_key}...\")\n",
        "\n",
        "            try:\n",
        "                s3.upload_file(local_path, bucket, s3_key)\n",
        "                print(f\"Successfully uploaded: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to upload {filename}. Error: {e}\")\n",
        "\n",
        "upload_local_directory_to_s3(local_dir, bucket_name, s3_prefix)\n"
      ],
      "metadata": {
        "id": "0w8tkfRQWFHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}